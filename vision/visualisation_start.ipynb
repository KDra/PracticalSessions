{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J7p406abzgl"
   },
   "source": [
    "## Part II: Visualise saliency maps\n",
    "- Import an already trained baseline model.\n",
    "- Visualise the gradients of class probabilities w.r.t inputs to obtain saliency maps.\n",
    "- Generate inputs that maximise class probabilities.\n",
    "\n",
    "#### Exercises:\n",
    "\n",
    "1. Retrieve the gradient of the most probable class w.r.t. to input image using `tf.gradients` and plot saliency maps.\n",
    "2. Iterate the above and take steps into the direction of this gradient starting from a test image.\n",
    "\n",
    ">*  The gradient indicates how to modify the input image to make it look more like the class it is taken from, according to the network.\n",
    ">* Note that the network weights are kept fixed, only the input is transformed, i.e. we retrieve gradients, but we never apply them to the network weights.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhWI4Pix5GJw"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "na0VvPXmYKp1"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "# we will use Sonnet on top of TF \n",
    "!pip install -q dm-sonnet\n",
    "import sonnet as snt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library.\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xlKHOLbhvY7"
   },
   "outputs": [],
   "source": [
    "# Reset graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5c0b2N7PZamq"
   },
   "outputs": [],
   "source": [
    "# Display function\n",
    "class_mapping = [u'airplane', u'automobile', u'bird', u'cat', u'deer', \n",
    "                 u'dog', u'frog', u'horse', u'ship', u'truck']\n",
    "def gallery(maps, imgs, pclass, gt, scale=4.0):\n",
    "  num_images= maps.shape[0]\n",
    "  maps = np.abs(maps).mean(axis=-1)\n",
    "  ff, axes = plt.subplots(2, num_images,\n",
    "                          subplot_kw={'xticks': [], \n",
    "                                      'yticks': []})\n",
    "  for i in range(0, num_images):\n",
    "    tt_pred = class_mapping[pclass[i]]\n",
    "    tt_gt = class_mapping[gt[i]]\n",
    "    mm = maps[i]/np.amax(maps[i])\n",
    "    mm_rescale = rescale(mm, scale)                         \n",
    "    axes[0,i].imshow(mm_rescale)\n",
    "    img = (imgs[i]+1.0)/2.0\n",
    "    img_rescale = rescale(img, scale)\n",
    "    axes[1,i].imshow(img_rescale)\n",
    "    plt.setp(axes[0,i].get_xticklabels(), visible=False)\n",
    "    plt.setp(axes[0,i].get_yticklabels(), visible=False)\n",
    "    axes[0,i].set_title('pred={}'.format(tt_pred))\n",
    "    axes[1,i].set_title('gt={}'.format(tt_gt))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NH0yiib_z4R8"
   },
   "source": [
    "### Copy the pretrained weights of baseline model on the virtual machine\n",
    "- you need to load all three files from the *baseline* folder (it will take about 5 minutes)\n",
    "- this loads a model with the same architecture that you defined earlier, but fully trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cubpPmHgECbc"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(uploaded)\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8g16XweXs2Uq"
   },
   "source": [
    "### Get dataset to be used for visualisation\n",
    "- Cifar-10 equivalent of MNIST for natural RGB images\n",
    "- 60000 32x32 colour images in 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "- train: 50000; test: 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1g_EOx07s1XZ"
   },
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "# (down)load dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHAggitWu94_"
   },
   "source": [
    "### Retrieve batches from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZofMjOuUEOF"
   },
   "outputs": [],
   "source": [
    "# define dimension of the batches to sample from the datasets\n",
    "BATCH_SIZE_TEST = 5 #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO1xULtaWqR6"
   },
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "batched_dataset_test = dataset_test.repeat().batch(BATCH_SIZE_TEST)\n",
    "iterator_test = batched_dataset_test.make_one_shot_iterator() \n",
    "(batch_test_images, batch_test_labels) = iterator_test.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IqxUPlmtLJX"
   },
   "source": [
    "### Model on which we will run the visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2scBoc09ZsO4"
   },
   "outputs": [],
   "source": [
    "class Baseline(snt.AbstractModule):\n",
    "  \n",
    "  def __init__(self, num_classes, name=\"baseline\"):\n",
    "    super(Baseline, self).__init__(name=name)\n",
    "    self._num_classes = num_classes\n",
    "    self._output_channels = [\n",
    "        64, 64, 128, 128, 128, 256, 256, 256, 512, 512, 512\n",
    "        ]\n",
    "    self._num_layers = len(self._output_channels)\n",
    "\n",
    "    self._kernel_shapes = [[3, 3]] * self._num_layers  # All kernels are 3x3.\n",
    "    self._strides = [1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1]\n",
    "    self._paddings = [snt.SAME] * self._num_layers\n",
    "   \n",
    "  def _build(self, inputs, is_training=None, test_local_stats=False):\n",
    "    net = inputs\n",
    "    # instantiate all the convolutional layers \n",
    "    layers = [snt.Conv2D(name=\"conv_2d_{}\".format(i),\n",
    "                         output_channels=self._output_channels[i],\n",
    "                         kernel_shape=self._kernel_shapes[i],\n",
    "                         stride=self._strides[i],\n",
    "                         padding=self._paddings[i],\n",
    "                         use_bias=True) for i in xrange(self._num_layers)]\n",
    "    # connect them to the graph, adding batch norm and non-linearity\n",
    "    for i, layer in enumerate(layers):\n",
    "      net = layer(net)\n",
    "      bn = snt.BatchNorm(name=\"batch_norm_{}\".format(i))\n",
    "      net = bn(net, is_training=is_training, test_local_stats=test_local_stats)\n",
    "      net = tf.nn.relu(net)\n",
    "\n",
    "    net = tf.reduce_mean(net, reduction_indices=[1, 2], keepdims=False,\n",
    "                         name=\"avg_pool\")\n",
    "\n",
    "    logits = snt.Linear(self._num_classes)(net)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZzlpO0oJFZy"
   },
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scUEks38XHQX"
   },
   "outputs": [],
   "source": [
    "# Test preprocessing: only scale to [-1,1].\n",
    "def test_image_preprocess():\n",
    "  def fn(image):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = image * 2.0 - 1.0\n",
    "    return image\n",
    "  return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c68u4b86uNb8"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model \n",
    "with tf.variable_scope(\"baseline\"):\n",
    "  model = Baseline(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grtPyN7QgIsP"
   },
   "outputs": [],
   "source": [
    "# Connect the model to data\n",
    "preprocess_op = test_image_preprocess()\n",
    "batch_test_images = preprocess_op(batch_test_images)\n",
    "test_predictions = model(batch_test_images, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AWtWWr8xgSCN"
   },
   "outputs": [],
   "source": [
    "# Create saver to restore the pre-trained model\n",
    "# First remove the scope name from variables name, since the name in the checkpoint doesn't include it\n",
    "var_list = snt.get_variables_in_scope(\"baseline\", \n",
    "                                      collection=tf.GraphKeys.GLOBAL_VARIABLES)  \n",
    "var_map = {}\n",
    "for i in range(0, len(var_list)):\n",
    "  name = var_list[i].name[len(\"baseline/\"):-2]\n",
    "  var_map[name] = var_list[i]\n",
    "  \n",
    "saver = tf.train.Saver(var_map, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLtMkv8htou9"
   },
   "outputs": [],
   "source": [
    "# For evaluation, we look at top_k_accuracy since it's easier to interpret; normally k=1 or k=5\n",
    "def top_k_accuracy(k, labels, logits):\n",
    "  in_top_k = tf.nn.in_top_k(predictions=tf.squeeze(logits), \n",
    "                            targets=tf.squeeze(tf.cast(labels, tf.int32)), k=k)\n",
    "  return tf.reduce_mean(tf.cast(in_top_k, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-FlGBzUKC1V"
   },
   "outputs": [],
   "source": [
    "test_acc = top_k_accuracy(1, batch_test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6IIjdk8LlAq"
   },
   "source": [
    "### Visualise saliency maps\n",
    "\n",
    "- We retrieve gradients w.r.t. inputs to obtain a saliency map over the input pixels, i.e. to understand which pixels in an image caused a certain output logit to be maximised.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGQF727ALo0s"
   },
   "outputs": [],
   "source": [
    "# Get the maximum output prediction\n",
    "maximum_prediction =  ############## YOUR CODE ##############\n",
    "\n",
    "# Get the gradient w.r.t. input images\n",
    "saliency_op = ############## YOUR CODE ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kmn15p7HP7Sy"
   },
   "outputs": [],
   "source": [
    "# Get the predicted class index for visualisation purposes.\n",
    "pred_class_op = ############## YOUR CODE ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lt_G1gTN3w-W"
   },
   "outputs": [],
   "source": [
    "# Create the session and initialize variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RhJx4LPJbEjf"
   },
   "outputs": [],
   "source": [
    "# Restore pre-trained weights\n",
    "saver.restore(sess, \"baseline.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVrXDQFcK-KO"
   },
   "outputs": [],
   "source": [
    "# Check if import was done correctly by running eval on cifar test set\n",
    "# expected_accuracy = 0.94\n",
    "num_batches = 1000  # 1000 batches * 5 samples per batch = 5000\n",
    "avg_accuracy = 0.0\n",
    "for _ in range(num_batches):\n",
    "  accuracy = sess.run(test_acc)\n",
    "  avg_accuracy += accuracy\n",
    "avg_accuracy /= num_batches\n",
    "\n",
    "print (\"Accuracy {:.3f}\".format(avg_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEfs4eYE88zS"
   },
   "outputs": [],
   "source": [
    "# Get saliency maps\n",
    "smap, inp_img, predicted_class, ground_truth = sess.run(\n",
    "    [saliency_op, batch_test_images, \n",
    "     pred_class_op, tf.squeeze(batch_test_labels)])\n",
    "\n",
    "# Display \n",
    "gallery(smap, inp_img, predicted_class, ground_truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZ0eMrq7hFIV"
   },
   "source": [
    "### Not that impressive, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sG98vwnxdL6c"
   },
   "source": [
    "### Let's generate the image that maximises the probability of a given class $c$\n",
    "\n",
    "The previous exercise computed\n",
    "$$\n",
    "\\frac{\\partial y_{c}}{\\partial x}\n",
    "$$\n",
    "\n",
    "Now we modify $x$ to search for $\\hat x$ that maximises $\\frac{\\partial y_{c}}{\\partial x}$ using an iterative gradient-descent like approach:\n",
    "\n",
    "$$\n",
    "x_{t+1} = \\min(1, \\max(-1, x_t + \\alpha \\frac{\\partial y_{c}}{\\partial x})), t \\in \\{0, N\\}\n",
    "$$\n",
    "$$\n",
    "x_0 = \\text{initial test image from class } c \n",
    "$$\n",
    "\n",
    "Use e.g. $\\alpha = 0.1$ and $N=10000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKr6H4O-ZIku"
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "N = 10000\n",
    "\n",
    "# get saliency maps\n",
    "smap, inp_img, predicted_class, ground_truth = sess.run(\n",
    "      [saliency_op, batch_test_images, \n",
    "       pred_class_op, tf.squeeze(batch_test_labels)])\n",
    "\n",
    "for t in range(N):\n",
    "  #############\n",
    "  #           #\n",
    "  # YOUR CODE #\n",
    "  #           #\n",
    "  #############\n",
    "  \n",
    "  # display transformed input image at every 1000 iterations\n",
    "  if t % 1000 == 0:\n",
    "    print ('Transformed input at iter {0:5d} out of {1:5d}'.format(int(t), int(N)))\n",
    "    gallery(smap, inp_img, predicted_class, ground_truth)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "visualisation_start.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
